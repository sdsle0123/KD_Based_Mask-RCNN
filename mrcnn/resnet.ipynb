{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as KL\n",
    "import tensorflow.keras.utils as KU\n",
    "from tensorflow.python.eager import context\n",
    "import tensorflow.keras.models as KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(KL.BatchNormalization):\n",
    "    \"\"\"Extends the Keras BatchNormalization class to allow a central place\n",
    "    to make changes if needed.\n",
    "\n",
    "    Batch normalization has a negative effect on training if batches are small\n",
    "    so this layer is often frozen (via setting in Config class) and functions\n",
    "    as linear layer.\n",
    "    \"\"\"\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Note about training values:\n",
    "            None: Train BN layers. This is the normal mode\n",
    "            False: Freeze BN layers. Good when batch size is small\n",
    "            True: (don't use). Set layer in training mode even when making inferences\n",
    "        \"\"\"\n",
    "        return super(self.__class__, self).call(inputs, training=training)\n",
    "\n",
    "\n",
    "def compute_backbone_shapes(config, image_shape):\n",
    "    \"\"\"Computes the width and height of each stage of the backbone network.\n",
    "\n",
    "    Returns:\n",
    "        [N, (height, width)]. Where N is the number of stages\n",
    "    \"\"\"\n",
    "    if callable(config.BACKBONE):\n",
    "        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n",
    "\n",
    "    # Currently supports ResNet only\n",
    "    assert config.BACKBONE in [\"resnet34\", \"resnet50\", \"resnet101\"]\n",
    "    return np.array(\n",
    "        [[int(math.ceil(image_shape[0] / stride)),\n",
    "            int(math.ceil(image_shape[1] / stride))]\n",
    "            for stride in config.BACKBONE_STRIDES])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idendity Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block,\n",
    "                   use_bias=True, train_bn=True):\n",
    "    \"\"\"The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        use_bias: Boolean. To use or not use a bias in conv layers.\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    \"\"\"\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a',\n",
    "                  use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c',\n",
    "                  use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, input_tensor])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "    return x\n",
    "\n",
    "def identity_block0(input_tensor, kernel_size, filters, stage, block,\n",
    "                   use_bias=True, train_bn=True):\n",
    "\n",
    "    nb_filter1, nb_filter2 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv2D(nb_filter1, (kernel_size, kernel_size), name=conv_name_base + '2a', padding='same',\n",
    "                  use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, input_tensor])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block,\n",
    "               strides=(2, 2), use_bias=True, train_bn=True):\n",
    "    \"\"\"conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        use_bias: Boolean. To use or not use a bias in conv layers.\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    \"\"\"\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n",
    "                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n",
    "                  '2c', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n",
    "\n",
    "    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n",
    "                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n",
    "    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, shortcut])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block0(input_tensor, kernel_size, filters, stage, block,\n",
    "                   strides, use_bias=True, train_bn=True):\n",
    "\n",
    "    nb_filter1, nb_filter2 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv2D(nb_filter1, (kernel_size, kernel_size), padding='same', strides=strides,\n",
    "                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "\n",
    "    shortcut = KL.Conv2D(nb_filter2, (1,1), strides=strides, padding='same',\n",
    "                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n",
    "    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, shortcut])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_graph(input_image, architecture, stage5=False, train_bn=True):\n",
    "    \"\"\"Build a ResNet graph.\n",
    "        architecture: Can be resnet50 or resnet101\n",
    "        stage5: Boolean. If False, stage5 of the network is not created\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    \"\"\"\n",
    "    assert architecture in [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\"]\n",
    "    block_identify = {\"resnet18\":0, \"resnet34\":1, \"resnet50\":2, \"resnet101\":2}[architecture]\n",
    "\n",
    "    # Stage 1\n",
    "    x = KL.ZeroPadding2D((3, 3))(input_image)\n",
    "    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)\n",
    "    x = BatchNorm(name='bn_conv1')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    # Stage 2\n",
    "    if block_identify == 1 :\n",
    "        x = conv_block0(x, 3, [64, 64], stage=2, block='a', strides=(1, 1), train_bn=train_bn)\n",
    "        x = identity_block0(x, 3, [64, 64], stage=2, block='b', train_bn=train_bn)\n",
    "        C2 = x = identity_block0(x, 3, [64, 64], stage=2, block='c', train_bn=train_bn)\n",
    "    elif block_identify == 0 :\n",
    "        x = conv_block0(x, 3, [64, 64], stage=2, block='a', strides=(1, 1), train_bn=train_bn)\n",
    "        C2 = x = identity_block0(x, 3, [64, 64], stage=2, block='b', train_bn=train_bn)\n",
    "    else:\n",
    "        x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)\n",
    "        x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)\n",
    "        C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)\n",
    "\n",
    "    # Stage 3\n",
    "    if block_identify == 1 :\n",
    "        x = conv_block0(x, 3, [128, 128], stage=3, block='a', strides=(2, 2), train_bn=train_bn)\n",
    "        x = identity_block0(x, 3, [128, 128], stage=3, block='b', train_bn=train_bn)\n",
    "        x = identity_block0(x, 3, [128, 128], stage=3, block='c', train_bn=train_bn)\n",
    "        C3 = x = identity_block0(x, 3, [128, 128], stage=3, block='d', train_bn=train_bn)\n",
    "    elif block_identify == 0 :\n",
    "        x = conv_block0(x, 3, [128, 128], stage=3, block='a', strides=(2, 2), train_bn=train_bn)\n",
    "        C3 = x = identity_block0(x, 3, [128, 128], stage=3, block='b', train_bn=train_bn)\n",
    "    else:\n",
    "        x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)\n",
    "        C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)\n",
    "\n",
    "    # Stage 4\n",
    "    block_count = {\"resnet18\":1, \"resnet34\": 5,\"resnet50\": 5, \"resnet101\": 22}[architecture]\n",
    "    if block_identify == 1:\n",
    "        x = conv_block0(x, 3, [256, 256], stage=4, block='a', strides=(2, 2), train_bn=train_bn)\n",
    "        for i in range(block_count):\n",
    "            x = identity_block0(x, 3, [256, 256], stage=4, block=chr(98 + i), train_bn=train_bn)\n",
    "        C4 =  x\n",
    "    elif block_identify == 0 :\n",
    "        x = conv_block0(x, 3, [256, 256], stage=4, block='a', strides=(2, 2), train_bn=train_bn)\n",
    "        for i in range(block_count):\n",
    "            x = identity_block0(x, 3, [256, 256], stage=4, block=chr(98 + i), train_bn=train_bn)\n",
    "        C4 =  x \n",
    "    else:\n",
    "        x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)\n",
    "        for i in range(block_count):\n",
    "            x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn)\n",
    "        C4 = x\n",
    "\n",
    "    # Stage 5\n",
    "    if stage5:\n",
    "        if block_identify == 1:\n",
    "            x = conv_block0(x, 3, [512, 512], stage=5, block='a',strides=(2, 2), train_bn=train_bn)\n",
    "            x = identity_block0(x, 3, [512, 512], stage=5, block='b', train_bn=train_bn)\n",
    "            C5 = x = identity_block0(x, 3, [512, 512], stage=5, block='c', train_bn=train_bn)\n",
    "        elif block_identify == 0 :\n",
    "            x = conv_block0(x, 3, [512, 512], stage=5, block='a', strides=(2, 2), train_bn=train_bn)\n",
    "            C5 = x = identity_block0(x, 3, [512, 512], stage=5, block='b', train_bn=train_bn)    \n",
    "        else:\n",
    "            x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)\n",
    "            x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)\n",
    "            C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)\n",
    "    else:\n",
    "        C5 = None\n",
    "    return [C1, C2, C3, C4, C5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = KL.Input(shape=[1024, 1024, 3], name=\"input_image\")\n",
    "\n",
    "output = resnet_graph(input_image, \"resnet18\",stage5=True, train_bn=True)\n",
    "\n",
    "model = KM.Model(input_image,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 1024, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPadding2 (None, 1030, 1030, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 64) 9472        zero_padding2d_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNorm)            (None, 512, 512, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_485 (Activation)     (None, 512, 512, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 256, 256, 64) 0           activation_485[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 256, 256, 64) 36928       max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNorm)       (None, 256, 256, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_486 (Activation)     (None, 256, 256, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 256, 256, 64) 36928       activation_486[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 256, 256, 64) 4160        max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNorm)       (None, 256, 256, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNorm)        (None, 256, 256, 64) 256         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_330 (Add)                   (None, 256, 256, 64) 0           bn2a_branch2b[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_out (Activation)          (None, 256, 256, 64) 0           add_330[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 256, 256, 64) 36928       res2a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNorm)       (None, 256, 256, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_487 (Activation)     (None, 256, 256, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 256, 256, 64) 36928       activation_487[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNorm)       (None, 256, 256, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_331 (Add)                   (None, 256, 256, 64) 0           bn2b_branch2b[0][0]              \n",
      "                                                                 res2a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2b_out (Activation)          (None, 256, 256, 64) 0           add_331[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 128, 128, 128 73856       res2b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNorm)       (None, 128, 128, 128 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_488 (Activation)     (None, 128, 128, 128 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 128, 128, 128 147584      activation_488[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 128, 128, 128 8320        res2b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNorm)       (None, 128, 128, 128 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNorm)        (None, 128, 128, 128 512         res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_332 (Add)                   (None, 128, 128, 128 0           bn3a_branch2b[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_out (Activation)          (None, 128, 128, 128 0           add_332[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 128, 128, 128 147584      res3a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNorm)       (None, 128, 128, 128 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_489 (Activation)     (None, 128, 128, 128 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 128, 128, 128 147584      activation_489[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNorm)       (None, 128, 128, 128 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_333 (Add)                   (None, 128, 128, 128 0           bn3b_branch2b[0][0]              \n",
      "                                                                 res3a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3b_out (Activation)          (None, 128, 128, 128 0           add_333[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 64, 64, 256)  295168      res3b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNorm)       (None, 64, 64, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_490 (Activation)     (None, 64, 64, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 64, 64, 256)  590080      activation_490[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 64, 64, 256)  33024       res3b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNorm)       (None, 64, 64, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNorm)        (None, 64, 64, 256)  1024        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_334 (Add)                   (None, 64, 64, 256)  0           bn4a_branch2b[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res4a_out (Activation)          (None, 64, 64, 256)  0           add_334[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 64, 64, 256)  590080      res4a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNorm)       (None, 64, 64, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_491 (Activation)     (None, 64, 64, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 64, 64, 256)  590080      activation_491[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNorm)       (None, 64, 64, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_335 (Add)                   (None, 64, 64, 256)  0           bn4b_branch2b[0][0]              \n",
      "                                                                 res4a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4b_out (Activation)          (None, 64, 64, 256)  0           add_335[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 32, 32, 512)  1180160     res4b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNorm)       (None, 32, 32, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_492 (Activation)     (None, 32, 32, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 32, 32, 512)  2359808     activation_492[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      res4b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNorm)       (None, 32, 32, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNorm)        (None, 32, 32, 512)  2048        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_336 (Add)                   (None, 32, 32, 512)  0           bn5a_branch2b[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res5a_out (Activation)          (None, 32, 32, 512)  0           add_336[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 32, 32, 512)  2359808     res5a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNorm)       (None, 32, 32, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_493 (Activation)     (None, 32, 32, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 32, 32, 512)  2359808     activation_493[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNorm)       (None, 32, 32, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_337 (Add)                   (None, 32, 32, 512)  0           bn5b_branch2b[0][0]              \n",
      "                                                                 res5a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5b_out (Activation)          (None, 32, 32, 512)  0           add_337[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,195,328\n",
      "Trainable params: 11,185,600\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
